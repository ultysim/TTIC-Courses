{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train_data, trcnt = utils.load_data_onechar('data/ptb.train.txt', False)\n",
    "valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.txt', False)\n",
    "test_data, tecnt = utils.load_data_onechar('data/ptb.test.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 49.13856 Avg loss = 3.92808\n",
      "Initial generated sentence \n",
      "the agreements bringwx*//659e&@$$vl777#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u\n",
      "Epoch 0: Perplexity: 7.97855 Avg loss = 2.10561 [29.985 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 1: Perplexity: 6.34002 Avg loss = 1.87493 [24.513 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bring the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said t\n",
      "Epoch 2: Perplexity: 5.46314 Avg loss = 1.72519 [18.872 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bring the company said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said the said th\n",
      "Epoch 3: Perplexity: 4.92069 Avg loss = 1.61944 [20.575 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements brings and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the company the stock and the compan\n",
      "Epoch 4: Perplexity: 4.60217 Avg loss = 1.55068 [19.447 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bring the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the company the com\n",
      "Epoch 5: Perplexity: 4.36111 Avg loss = 1.49666 [17.053 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements brings and the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said the company said \n",
      "Epoch 6: Perplexity: 4.14795 Avg loss = 1.44756 [16.859 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bring the company and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <un\n",
      "Epoch 7: Perplexity: 4.01104 Avg loss = 1.41582 [16.791 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements brings and the company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company\n",
      "Epoch 8: Perplexity: 3.97807 Avg loss = 1.40253 [17.017 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements brings are n't see the company 's n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n n to n\n",
      "Epoch 9: Perplexity: 3.83462 Avg loss = 1.37039 [17.223 mins]\n",
      "Epoch 9: generated sentence \n",
      "the agreements bringing the company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company 's company '\n",
      "Epoch 10: Perplexity: 3.76121 Avg loss = 1.35132 [17.484 mins]\n",
      "Epoch 10: generated sentence \n",
      "the agreements bring a <unk> the company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange com\n",
      "Epoch 11: Perplexity: 3.71332 Avg loss = 1.33702 [17.238 mins]\n",
      "Epoch 11: generated sentence \n",
      "the agreements bring in the state of the <unk> <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <un\n",
      "Epoch 12: Perplexity: 3.66092 Avg loss = 1.32275 [17.354 mins]\n",
      "Epoch 12: generated sentence \n",
      "the agreements bring a company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company said the company 's stock exchange company\n",
      "Epoch 13: Perplexity: 3.64476 Avg loss = 1.31753 [17.388 mins]\n",
      "Epoch 13: generated sentence \n",
      "the agreements brings and the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state in the state \n",
      "Epoch 14: Perplexity: 3.57080 Avg loss = 1.29980 [17.702 mins]\n",
      "Epoch 14: generated sentence \n",
      "the agreements bring a <unk> of the stock market said the company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange comp\n",
      "Epoch 15: Perplexity: 3.54852 Avg loss = 1.29233 [17.645 mins]\n",
      "Epoch 15: generated sentence \n",
      "the agreements bring a <unk> <unk> and the company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock ex\n",
      "Epoch 16: Perplexity: 3.63577 Avg loss = 1.31138 [17.799 mins]\n",
      "Epoch 16: generated sentence \n",
      "the agreements bringing and <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "Epoch 17: Perplexity: 3.51127 Avg loss = 1.28235 [17.489 mins]\n",
      "Epoch 17: generated sentence \n",
      "the agreements bring and the company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company\n",
      "Epoch 18: Perplexity: 3.49990 Avg loss = 1.27661 [17.904 mins]\n",
      "Epoch 18: generated sentence \n",
      "the agreements bringing and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <u\n",
      "Epoch 19: Perplexity: 3.47091 Avg loss = 1.26936 [17.863 mins]\n",
      "Epoch 19: generated sentence \n",
      "the agreements bring and the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock exchange companies are the company 's stock ex\n",
      "Epoch 20: Perplexity: 3.45761 Avg loss = 1.26428 [17.863 mins]\n",
      "Epoch 20: generated sentence \n",
      "the agreements bring and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <\n",
      "Epoch 21: Perplexity: 3.43147 Avg loss = 1.25758 [17.824 mins]\n",
      "Epoch 21: generated sentence \n",
      "the agreements bring and the company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company\n",
      "Epoch 22: Perplexity: 3.42102 Avg loss = 1.25457 [17.803 mins]\n",
      "Epoch 22: generated sentence \n",
      "the agreements bring and the stock exchange companies are the securities are the <unk> and the stock exchange companies are the securities are the <unk> and the stock exchange companies are the securities are the <unk> and the stock exchange companies are the securities are the <unk> and the stock exchange companies are the securities are the <unk> and the stock exchange companies are the securiti\n",
      "Epoch 23: Perplexity: 3.38501 Avg loss = 1.24430 [17.671 mins]\n",
      "Epoch 23: generated sentence \n",
      "the agreements bringing and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <unk> and <u\n",
      "Epoch 24: Perplexity: 3.36922 Avg loss = 1.23965 [17.300 mins]\n",
      "Epoch 24: generated sentence \n",
      "the agreements bringing and the company 's state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state \n",
      "Epoch 25: Perplexity: 3.36481 Avg loss = 1.23755 [17.370 mins]\n",
      "Epoch 25: generated sentence \n",
      "the agreements bringing company said the company 's <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <u\n",
      "Epoch 26: Perplexity: 3.34323 Avg loss = 1.23218 [17.556 mins]\n",
      "Epoch 26: generated sentence \n",
      "the agreements bring and the company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company 's stock exchange company\n",
      "Epoch 27: Perplexity: 3.33205 Avg loss = 1.22860 [17.680 mins]\n",
      "Epoch 27: generated sentence \n",
      "the agreements bringing company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state of the company 's state\n",
      "Epoch 28: Perplexity: 3.32909 Avg loss = 1.22606 [17.673 mins]\n",
      "Epoch 28: generated sentence \n",
      "the agreements bringing company said the company 's state of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of the n n share of\n",
      "Epoch 29: Perplexity: 3.41982 Avg loss = 1.25168 [17.677 mins]\n",
      "Epoch 29: generated sentence \n",
      "the agreements bring competitive comment}\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "\n",
    "def LSTMCell(xt, h, c):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wf), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wi), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wo), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.VDot(edf.ConCat(xt, h), Wc), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edf' from 'edf.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 182.90955 Avg loss = 5.27921\n",
      "Initial generated sentence \n",
      "the agreements bringaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "Epoch 0: Perplexity: 9.53933 Avg loss = 2.28844 [32.666 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring> .d .d .d .d .d .d .d .d .data nd> .datat}\n",
      "Epoch 1: Perplexity: 8.73949 Avg loss = 2.20002 [27.502 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bringvt. .dst.datk> .dvtk> .ddyvtvtvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtvtk> .dvtvtk> .dvtk> .dvtvtk> .dvtv\n",
      "Epoch 2: Perplexity: 8.90574 Avg loss = 2.21570 [28.170 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bringes.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s\n",
      "Epoch 3: Perplexity: 7.45768 Avg loss = 2.03839 [27.574 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bringvdasoddyvyvyvesodayvesodayvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvyvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvyvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvyvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvyvesodayvesodayvesodayvesodayvesodayvdayvyvyvesodayvyvesodayv\n",
      "Epoch 4: Perplexity: 7.26501 Avg loss = 2.01184 [31.927 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bringa}\n",
      "Epoch 5: Perplexity: 6.81700 Avg loss = 1.94766 [36.197 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements bringo r.da}\n",
      "Epoch 6: Perplexity: 6.62399 Avg loss = 1.91936 [27.470 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bringa}\n",
      "Epoch 7: Perplexity: 6.56828 Avg loss = 1.91167 [26.663 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements bringaa}\n",
      "Epoch 8: Perplexity: 6.75890 Avg loss = 1.93508 [27.046 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements bringast}\n",
      "Epoch 9: Perplexity: 6.32078 Avg loss = 1.87025 [26.971 mins]\n",
      "Epoch 9: generated sentence \n",
      "the agreements bringdadddat. ...t }\n",
      "Epoch 10: Perplexity: 6.25891 Avg loss = 1.86189 [26.913 mins]\n",
      "Epoch 10: generated sentence \n",
      "the agreements bringa..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t.t..t.t.t.t.t.t.\n",
      "Epoch 11: Perplexity: 6.25789 Avg loss = 1.86229 [27.514 mins]\n",
      "Epoch 11: generated sentence \n",
      "the agreements bringata.t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t..t\n",
      "Epoch 12: Perplexity: 6.22857 Avg loss = 1.85680 [27.000 mins]\n",
      "Epoch 12: generated sentence \n",
      "the agreements bringaa .kat..ainkdaaat..at..ainkdainkdaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atankaaaat..atan\n",
      "Epoch 13: Perplexity: 6.19905 Avg loss = 1.85195 [26.884 mins]\n",
      "Epoch 13: generated sentence \n",
      "the agreements bringaa}\n",
      "Epoch 14: Perplexity: 6.13578 Avg loss = 1.84465 [27.449 mins]\n",
      "Epoch 14: generated sentence \n",
      "the agreements bringanyankainkaaaaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatankanaaatankanysaaatank\n",
      "Epoch 15: Perplexity: 6.16457 Avg loss = 1.84764 [26.892 mins]\n",
      "Epoch 15: generated sentence \n",
      "the agreements bringaaatanananananananananananananananananaaatananananananananananaaatananananananananananaaatanananananananananaaatananananananananananaaatanananananananananaaatananananananananananaaatanananananananananaaatananananananananananaaatanananananananananaaatananananananananananaaatanananananananananaaatananananananananananaaatanananananananananaaatananananananananananaaatananananananan\n",
      "Epoch 16: Perplexity: 6.15293 Avg loss = 1.84361 [27.002 mins]\n",
      "Epoch 16: generated sentence \n",
      "the agreements bringain}\n",
      "Epoch 17: Perplexity: 6.12674 Avg loss = 1.84165 [27.193 mins]\n",
      "Epoch 17: generated sentence \n",
      "the agreements bringanananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananan\n",
      "Epoch 18: Perplexity: 5.94594 Avg loss = 1.80971 [27.795 mins]\n",
      "Epoch 18: generated sentence \n",
      "the agreements bringaaats ..m ..m ..m ..mats ..m ..m ..m ..mats ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m \n",
      "Epoch 19: Perplexity: 5.97008 Avg loss = 1.81707 [28.016 mins]\n",
      "Epoch 19: generated sentence \n",
      "the agreements bringadats ......................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 20: Perplexity: 5.84898 Avg loss = 1.79501 [27.629 mins]\n",
      "Epoch 20: generated sentence \n",
      "the agreements bringaanaatananadatanananananaa }\n",
      "Epoch 21: Perplexity: 5.83404 Avg loss = 1.79299 [27.901 mins]\n",
      "Epoch 21: generated sentence \n",
      "the agreements bringaaatsaams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.saams s.\n",
      "Epoch 22: Perplexity: 5.78030 Avg loss = 1.78384 [27.796 mins]\n",
      "Epoch 22: generated sentence \n",
      "the agreements brings s.........................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 23: Perplexity: 5.60800 Avg loss = 1.75298 [28.257 mins]\n",
      "Epoch 23: generated sentence \n",
      "the agreements brings s.........................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 24: Perplexity: 5.62519 Avg loss = 1.75617 [28.910 mins]\n",
      "Epoch 24: generated sentence \n",
      "the agreements brings s.m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..m ..\n",
      "Epoch 25: Perplexity: 5.59400 Avg loss = 1.75102 [28.587 mins]\n",
      "Epoch 25: generated sentence \n",
      "the agreements brings s.s ......................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 26: Perplexity: 5.54470 Avg loss = 1.74233 [29.054 mins]\n",
      "Epoch 26: generated sentence \n",
      "the agreements bringsaandananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananana\n",
      "Epoch 27: Perplexity: 5.47191 Avg loss = 1.72945 [29.326 mins]\n",
      "Epoch 27: generated sentence \n",
      "the agreements brings s.........................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 28: Perplexity: 5.43731 Avg loss = 1.72354 [29.317 mins]\n",
      "Epoch 28: generated sentence \n",
      "the agreements brings s.ts......................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Epoch 29: Perplexity: 5.51776 Avg loss = 1.73732 [27.518 mins]\n",
      "Epoch 29: generated sentence \n",
      "the agreements brings s.tsnddaanyss s.tsndaanys s.snddaanyss s.snddaanyss s.snddaananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananananana\n"
     ]
    }
   ],
   "source": [
    "#Using momentum in batch norm\n",
    "\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm1.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 9.30435 Avg loss = 2.26632\n",
      "Initial generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 0: Perplexity: 9.05345 Avg loss = 2.23770 [30.059 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 1: Perplexity: 9.22198 Avg loss = 2.25798 [29.019 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 2: Perplexity: 9.31352 Avg loss = 2.26717 [30.502 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 3: Perplexity: 9.26186 Avg loss = 2.26127 [31.255 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using batch norm from paper\n",
    "\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm2.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNorm(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edf' from 'edf.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 20.10327 Avg loss = 3.05206\n",
      "Initial generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 0: Perplexity: 9.49633 Avg loss = 2.28494 [31.834 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 1: Perplexity: 9.13977 Avg loss = 2.24846 [28.378 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 2: Perplexity: 8.55601 Avg loss = 2.17924 [24.705 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 3: Perplexity: 8.06274 Avg loss = 2.12025 [25.031 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 4: Perplexity: 7.78266 Avg loss = 2.08381 [24.862 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 5: Perplexity: 7.68082 Avg loss = 2.06887 [24.999 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 6: Perplexity: 7.37134 Avg loss = 2.02961 [24.413 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 7: Perplexity: 7.28840 Avg loss = 2.01802 [24.194 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 8: Perplexity: 7.14088 Avg loss = 1.99440 [24.132 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 9: Perplexity: 7.07772 Avg loss = 1.98902 [24.269 mins]\n",
      "Epoch 9: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 10: Perplexity: 6.97407 Avg loss = 1.97377 [24.628 mins]\n",
      "Epoch 10: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 11: Perplexity: 6.93374 Avg loss = 1.96592 [25.239 mins]\n",
      "Epoch 11: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 12: Perplexity: 6.72461 Avg loss = 1.93697 [24.694 mins]\n",
      "Epoch 12: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 13: Perplexity: 6.81721 Avg loss = 1.95067 [24.790 mins]\n",
      "Epoch 13: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 14: Perplexity: 6.67143 Avg loss = 1.93056 [24.773 mins]\n",
      "Epoch 14: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 15: Perplexity: 6.56641 Avg loss = 1.91226 [24.561 mins]\n",
      "Epoch 15: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 16: Perplexity: 6.74987 Avg loss = 1.93901 [24.277 mins]\n",
      "Epoch 16: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 17: Perplexity: 6.59900 Avg loss = 1.92158 [24.178 mins]\n",
      "Epoch 17: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 18: Perplexity: 6.45832 Avg loss = 1.89550 [24.373 mins]\n",
      "Epoch 18: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 19: Perplexity: 6.44821 Avg loss = 1.89626 [24.000 mins]\n",
      "Epoch 19: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 20: Perplexity: 6.58458 Avg loss = 1.91697 [24.220 mins]\n",
      "Epoch 20: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 21: Perplexity: 6.44165 Avg loss = 1.89472 [23.918 mins]\n",
      "Epoch 21: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 22: Perplexity: 6.38832 Avg loss = 1.88721 [23.982 mins]\n",
      "Epoch 22: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 23: Perplexity: 6.32228 Avg loss = 1.87611 [24.101 mins]\n",
      "Epoch 23: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 24: Perplexity: 6.45062 Avg loss = 1.89676 [23.969 mins]\n",
      "Epoch 24: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 25: Perplexity: 6.30259 Avg loss = 1.87419 [24.324 mins]\n",
      "Epoch 25: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 26: Perplexity: 6.25746 Avg loss = 1.86537 [24.631 mins]\n",
      "Epoch 26: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 27: Perplexity: 6.22943 Avg loss = 1.86134 [24.742 mins]\n",
      "Epoch 27: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 28: Perplexity: 6.40355 Avg loss = 1.88886 [24.518 mins]\n",
      "Epoch 28: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 29: Perplexity: 6.27220 Avg loss = 1.86733 [24.984 mins]\n",
      "Epoch 29: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "#Using batch norm from online code\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm4.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNormPaper(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNormPaper(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNormPaper(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNormPaper(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edf' from 'edf.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 20.10327 Avg loss = 3.05206\n",
      "Initial generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 0: Perplexity: 9.49818 Avg loss = 2.28507 [40.724 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 1: Perplexity: 9.03764 Avg loss = 2.23690 [30.126 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 2: Perplexity: 8.59071 Avg loss = 2.18348 [28.841 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 3: Perplexity: 8.05384 Avg loss = 2.11912 [28.438 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 4: Perplexity: 7.78957 Avg loss = 2.08471 [28.324 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 5: Perplexity: 7.68977 Avg loss = 2.07005 [28.537 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 6: Perplexity: 7.39155 Avg loss = 2.03284 [35.172 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 7: Perplexity: 7.19391 Avg loss = 2.00521 [32.646 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 8: Perplexity: 7.16224 Avg loss = 1.99783 [34.098 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3e17f00bc9f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradClip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Simas\\Desktop\\School\\Deep Learning\\Final Project\\edf.py\u001b[0m in \u001b[0;36mBackward\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# Optimization functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Simas\\Desktop\\School\\Deep Learning\\Final Project\\edf.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[1;31m# step1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0mdx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[1;31m# step0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\winpython\\python-2.7.10.amd64\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \"\"\"\n\u001b[1;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Using batch norm from online code, time step mean and variance\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm3.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "timeHold = np.zeros((1000,2,hidden_dim))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, timeHold, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,timeHold,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,timeHold,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,timeHold,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,timeHold,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, timeHold[t], edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, timeHold[t], edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edf' from 'edf.py'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fold_data(data):\n",
    "    out = list()\n",
    "    long = len(data)\n",
    "    i = 0\n",
    "    while i < long/2:\n",
    "        hold = list()\n",
    "        hold.extend(data[i])\n",
    "        hold.extend(data[long-i-1])\n",
    "        out.append(np.array(hold))\n",
    "        i += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 7.08902 Avg loss = 1.96433\n",
      "Initial generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 0: Perplexity: 8.65026 Avg loss = 2.16442 [38.650 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 1: Perplexity: 7.74601 Avg loss = 2.05359 [38.804 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 2: Perplexity: 7.60941 Avg loss = 2.03499 [35.536 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 3: Perplexity: 7.12439 Avg loss = 1.96874 [33.632 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 4: Perplexity: 8.55604 Avg loss = 2.15270 [33.823 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 5: Perplexity: 6.90150 Avg loss = 1.93736 [33.499 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 6: Perplexity: 7.83218 Avg loss = 2.06458 [35.400 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 7: Perplexity: 6.96194 Avg loss = 1.94582 [35.575 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 8: Perplexity: 6.82587 Avg loss = 1.92616 [35.649 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 9: Perplexity: 7.94338 Avg loss = 2.07891 [34.606 mins]\n",
      "Epoch 9: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 10: Perplexity: 7.21319 Avg loss = 1.98168 [38.267 mins]\n",
      "Epoch 10: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 11: Perplexity: 7.06290 Avg loss = 1.96058 [40.622 mins]\n",
      "Epoch 11: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 12: Perplexity: 7.21029 Avg loss = 1.98142 [37.607 mins]\n",
      "Epoch 12: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 13: Perplexity: 6.92410 Avg loss = 1.94055 [37.874 mins]\n",
      "Epoch 13: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 14: Perplexity: 6.89781 Avg loss = 1.93648 [36.744 mins]\n",
      "Epoch 14: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 15: Perplexity: 6.80696 Avg loss = 1.92350 [37.447 mins]\n",
      "Epoch 15: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 16: Perplexity: 6.73758 Avg loss = 1.91314 [37.268 mins]\n",
      "Epoch 16: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 17: Perplexity: 6.69951 Avg loss = 1.90739 [37.217 mins]\n",
      "Epoch 17: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 18: Perplexity: 6.67190 Avg loss = 1.90326 [36.898 mins]\n",
      "Epoch 18: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 19: Perplexity: 6.64721 Avg loss = 1.89945 [36.104 mins]\n",
      "Epoch 19: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 20: Perplexity: 6.69362 Avg loss = 1.90640 [36.433 mins]\n",
      "Epoch 20: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 21: Perplexity: 6.62549 Avg loss = 1.89613 [34.407 mins]\n",
      "Epoch 21: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 22: Perplexity: 6.60862 Avg loss = 1.89352 [39.102 mins]\n",
      "Epoch 22: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 23: Perplexity: 6.59171 Avg loss = 1.89092 [34.188 mins]\n",
      "Epoch 23: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 24: Perplexity: 6.57929 Avg loss = 1.88906 [34.852 mins]\n",
      "Epoch 24: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 25: Perplexity: 6.65577 Avg loss = 1.90073 [34.343 mins]\n",
      "Epoch 25: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 26: Perplexity: 6.56544 Avg loss = 1.88689 [33.893 mins]\n",
      "Epoch 26: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 27: Perplexity: 6.54925 Avg loss = 1.88444 [34.595 mins]\n",
      "Epoch 27: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 28: Perplexity: 6.53527 Avg loss = 1.88228 [34.215 mins]\n",
      "Epoch 28: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 29: Perplexity: 6.52574 Avg loss = 1.88087 [34.394 mins]\n",
      "Epoch 29: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "#Using batch norm from online code, time step mean and variance\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm5.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "#folddata\n",
    "data_train = fold_data(train_data)\n",
    "data_valid = fold_data(valid_data)\n",
    "data_test = fold_data(test_data)\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "timeHold = np.zeros((1000,2,hidden_dim))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, timeHold, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,timeHold,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,timeHold,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,timeHold,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNormTime(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,timeHold,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, timeHold[t], edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, timeHold[t], edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(data_train), batch)\n",
    "minbatches = [data_train[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(data_valid, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(data_valid, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 50.17347 Avg loss = 3.93221\n",
      "Initial generated sentence \n",
      "the agreements bringwx*//655.77#*b//655.77#*b//655.77#*b//655.77#*b//655.77#*b//655.77#*b//65e   *bff*b//655.77}\n",
      "Epoch 0: Perplexity: 9.30988 Avg loss = 2.24059 [35.608 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bring the the the the the the the the the the the the the the the the the ther and the ther and the ther and the ther and the ther and the ther the the the the the the the the the the the the the the the the ther the the the ther and the ther the the ther ther ther ther ther ther ther ther ther ther ther ther ther ther ther therare thar the ther ther fore thare thar ther fore fofint\n",
      "Epoch 1: Perplexity: 8.07657 Avg loss = 2.09762 [40.758 mins]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edf.py:176: RuntimeWarning: overflow encountered in exp\n",
      "  self.value = 1. / (1. + np.exp(-self.x.value))\n",
      "edf.py:198: RuntimeWarning: overflow encountered in exp\n",
      "  x_neg_exp = np.exp(-self.x.value)\n",
      "edf.py:200: RuntimeWarning: invalid value encountered in divide\n",
      "  self.value = (x_exp - x_neg_exp)/(x_exp + x_neg_exp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 2: Perplexity: 6.71031 Avg loss = 1.91137 [38.617 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 3: Perplexity: 5.58694 Avg loss = 1.72640 [41.392 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Epoch 4: Perplexity: 6.94784 Avg loss = 1.94542 [49.722 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bring@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edf.py:197: RuntimeWarning: overflow encountered in exp\n",
      "  x_exp = np.exp(self.x.value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5201b314fbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradClip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0medf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Simas\\Desktop\\School\\Deep Learning\\Final Project\\edf.pyc\u001b[0m in \u001b[0;36mBackward\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# Optimization functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Simas\\Desktop\\School\\Deep Learning\\Final Project\\edf.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mdxhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0mdvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdxhat\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_normalized\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_var\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0mdmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdxhat\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdvar\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdxhat\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvar\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_mean\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdvar\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdmean\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Using batch norm from momentum, time step mean and variance\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm6.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "#folddata\n",
    "data_train = fold_data(train_data)\n",
    "data_valid = fold_data(valid_data)\n",
    "data_test = fold_data(test_data)\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "timeHold = np.zeros((1000,2,hidden_dim))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, timeHold, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNormMom(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,timeHold,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNormMom(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,timeHold,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNormMom(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,timeHold,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNormMom(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,timeHold,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, timeHold[t], edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, timeHold[t], edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(data_train), batch)\n",
    "minbatches = [data_train[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(data_valid, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(data_valid, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edf' from 'edf.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 50.17347 Avg loss = 3.93221\n",
      "Initial generated sentence \n",
      "the agreements bringwx0   @@>dmal77#*b8ffdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3fdx<p3\n",
      "Epoch 0: Perplexity: 9.30988 Avg loss = 2.24059 [35.878 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bringtattitittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotittotitto\n",
      "Epoch 1: Perplexity: 8.07657 Avg loss = 2.09762 [39.892 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bringtttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "Epoch 2: Perplexity: 6.71031 Avg loss = 1.91137 [39.252 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bringstorlssssssssssssstassssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssstasssssssssst\n",
      "Epoch 3: Perplexity: 5.58694 Avg loss = 1.72640 [39.161 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bringattttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "Epoch 4: Perplexity: 6.94784 Avg loss = 1.94542 [42.086 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bringssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "Epoch 5: Perplexity: 4.90409 Avg loss = 1.59515 [41.714 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements bringllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "Epoch 6: Perplexity: 4.76539 Avg loss = 1.56593 [43.389 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bringllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "Epoch 7: Perplexity: 4.39490 Avg loss = 1.48450 [43.439 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements bringttllltstltsttt}\n",
      "Epoch 8: Perplexity: 4.19061 Avg loss = 1.43689 [42.060 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements bringtlltstltstlt}\n",
      "Epoch 9: Perplexity: 4.08790 Avg loss = 1.41193 [44.719 mins]\n",
      "Epoch 9: generated sentence \n",
      "the agreements bringll}\n",
      "Epoch 10: Perplexity: 3.99543 Avg loss = 1.38908 [44.937 mins]\n",
      "Epoch 10: generated sentence \n",
      "the agreements bringll}\n",
      "Epoch 11: Perplexity: 3.90291 Avg loss = 1.36572 [47.787 mins]\n",
      "Epoch 11: generated sentence \n",
      "the agreements bringllatllst}\n",
      "Epoch 12: Perplexity: 3.83866 Avg loss = 1.34909 [45.333 mins]\n",
      "Epoch 12: generated sentence \n",
      "the agreements bringlatllatstl}\n",
      "Epoch 13: Perplexity: 3.79181 Avg loss = 1.33674 [45.839 mins]\n",
      "Epoch 13: generated sentence \n",
      "the agreements bringlatl}\n",
      "Epoch 14: Perplexity: 3.78743 Avg loss = 1.33501 [44.912 mins]\n",
      "Epoch 14: generated sentence \n",
      "the agreements bringallalallalallallallallallall}\n",
      "Epoch 15: Perplexity: 3.84822 Avg loss = 1.35077 [45.858 mins]\n",
      "Epoch 15: generated sentence \n",
      "the agreements bringl}\n",
      "Epoch 16: Perplexity: 3.75905 Avg loss = 1.32732 [48.316 mins]\n",
      "Epoch 16: generated sentence \n",
      "the agreements bringl}\n",
      "Epoch 17: Perplexity: 3.70715 Avg loss = 1.31360 [47.106 mins]\n",
      "Epoch 17: generated sentence \n",
      "the agreements bringl}\n",
      "Epoch 18: Perplexity: 3.75279 Avg loss = 1.32562 [45.078 mins]\n",
      "Epoch 18: generated sentence \n",
      "the agreements bringla}\n",
      "Epoch 19: Perplexity: 3.65235 Avg loss = 1.29872 [45.367 mins]\n",
      "Epoch 19: generated sentence \n",
      "the agreements bringlal}\n",
      "Epoch 20: Perplexity: 3.62260 Avg loss = 1.29056 [46.144 mins]\n",
      "Epoch 20: generated sentence \n",
      "the agreements bringlallallallallallallallallasallallallallallallallallallallallallallallallallallallallasasatrallallallallallallallallallallallallallallallasasatrallallallallallallallallallallallallallallallasasatrallallallallallallallallallallallallallallallasasatrallallallallallallallallallallallallallallallasasatrallallallallallallallallallallallallallallallasasatrallallallallallallallallallal\n",
      "Epoch 21: Perplexity: 3.60042 Avg loss = 1.28439 [45.751 mins]\n",
      "Epoch 21: generated sentence \n",
      "the agreements bringlallallallallallasallallallallallasallallallasallallallasallallallasallallallasallallallasallallallasallallasallallallasasa}\n",
      "Epoch 22: Perplexity: 3.57832 Avg loss = 1.27824 [50.769 mins]\n",
      "Epoch 22: generated sentence \n",
      "the agreements bringlallallallallasallallallallasallallallasallallallasallallallasallallasallallallasasal}\n",
      "Epoch 23: Perplexity: 3.56247 Avg loss = 1.27377 [53.300 mins]\n",
      "Epoch 23: generated sentence \n",
      "the agreements bringlallalallallasallallallasallallallasallallasallallasatrallalallallasallallasallallasallasallallasallasallallasatrallalallallasallallasallallasallasallallasallasallallasatrallalallallasallallasallallasallasallallasallasallallasatrallalallallasallallasallallasallasallallasallasallallasatrallalallallasallallasallallasallasallallasallasallallasatrallalallallasallallasallallasallasa\n",
      "Epoch 24: Perplexity: 3.54888 Avg loss = 1.26995 [53.182 mins]\n",
      "Epoch 24: generated sentence \n",
      "the agreements bringallallalallallasallallasallallasallallasallasallallasatrallalallasallallasallasallallasatrallalallasallallasallallasatrallalallasallallasallasallallasatrallalallasallallasallallasatrallalallasallallasallasallallasatrallalallasallallasallallasatrallalallasallallasallasallallasatrallalallasallallasallallasatrallalallasallallasallasallallasatrallalallasallallasallallasatrallalalla\n",
      "Epoch 25: Perplexity: 3.53348 Avg loss = 1.26552 [54.645 mins]\n",
      "Epoch 25: generated sentence \n",
      "the agreements bringlalallallasallalallasallallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasallallasatrallasallalallasall\n",
      "Epoch 26: Perplexity: 3.52192 Avg loss = 1.26223 [53.979 mins]\n",
      "Epoch 26: generated sentence \n",
      "the agreements bringlalallallasallalallasallalallasallallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatrallasallalallasallalallasatral\n",
      "Epoch 27: Perplexity: 3.51107 Avg loss = 1.25907 [52.606 mins]\n",
      "Epoch 27: generated sentence \n",
      "the agreements bringlalalallasallalalallasallalalallasallalasa}\n",
      "Epoch 28: Perplexity: 3.49691 Avg loss = 1.25501 [49.087 mins]\n",
      "Epoch 28: generated sentence \n",
      "the agreements bringalalallasallatratroralalallasallatralalallasallatratroralalallasallatralalasa}\n",
      "Epoch 29: Perplexity: 3.48973 Avg loss = 1.25297 [47.395 mins]\n",
      "Epoch 29: generated sentence \n",
      "the agreements bringlalallatratroralalallallasatallatralalasallatratroralalallasallatratroralalallallasatallatratroralalallallasatallatralalasallatratroralalallasallatratroralalallallasatallatratroralalallallasatallatralalasallatratroralalallasallatratroralalallallasatallatratroralalallallasatallatralalasallatratroralalallasallatratroralalallallasatallatratroralalallallasatallatralalasallatratrora\n"
     ]
    }
   ],
   "source": [
    "#Using batch norm from momentum, folded data\n",
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM_Norm7.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "#folddata\n",
    "data_train = fold_data(train_data)\n",
    "data_valid = fold_data(valid_data)\n",
    "data_test = fold_data(test_data)\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "#normalization, using xavier init\n",
    "gamma = edf.Param(np.ones((hidden_dim))/10.0)\n",
    "beta = edf.Param(np.zeros((hidden_dim)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V, gamma, beta])\n",
    "\n",
    "\n",
    "                    \n",
    "#Adding norm layer, c_t+1 -> norm(c_t+1)\n",
    "def LSTMCellNorm(xt, h, c, test):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.BatchNormMomNT(edf.VDot(edf.ConCat(xt, h), Wf),gamma,beta,test), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.BatchNormMomNT(edf.VDot(edf.ConCat(xt, h), Wi),gamma,beta,test), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.BatchNormMomNT(edf.VDot(edf.ConCat(xt, h), Wo),gamma,beta,test), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.BatchNormMomNT(edf.VDot(edf.ConCat(xt, h), Wc),gamma,beta,test), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    #h_next = edf.Mul(o, edf.Tanh(edf.BatchNorm(c_next,gamma,beta,test)))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "\n",
    "\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCellNorm(xt, h, c, edf.Value(False))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCellNorm(xt, h, c, edf.Value(True))\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(data_train), batch)\n",
    "minbatches = [data_train[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(data_valid, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(data_valid, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence \n",
      "the agreements bringoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervoververvorvervov\n"
     ]
    }
   ],
   "source": [
    "def LSTMCell(xt, h, c):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wf), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wi), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wo), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.VDot(edf.ConCat(xt, h), Wc), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "def PredictNew(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "# generate some text given the prefix and trained model\n",
    "prefix = 'the agreements bring'  \n",
    "generation = PredictNew(400, utils.to_idxs(prefix))\n",
    "print(\"Generated sentence \")\n",
    "print (utils.to_string(generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
